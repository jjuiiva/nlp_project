{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Описание-проекта\" data-toc-modified-id=\"Описание-проекта-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Описание проекта</a></span></li><li><span><a href=\"#Изучаем-данные\" data-toc-modified-id=\"Изучаем-данные-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Изучаем данные</a></span></li><li><span><a href=\"#Предобработка-данных\" data-toc-modified-id=\"Предобработка-данных-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Предобработка данных</a></span></li><li><span><a href=\"#Векторизация\" data-toc-modified-id=\"Векторизация-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Векторизация</a></span></li><li><span><a href=\"#Обучение-моделей\" data-toc-modified-id=\"Обучение-моделей-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Обучение моделей</a></span></li><li><span><a href=\"#Общие-выводы\" data-toc-modified-id=\"Общие-выводы-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Общие выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект: определение токсичных комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "**Цель работы:**  \n",
    "\n",
    "1. Провести предобработку данных: лемматизировать текст, убрать стоп-слова и прочие символы, и тд \n",
    "\n",
    "2. Обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "3. Постройте модель со значением метрики качества *F1* не меньше 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изучаем данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устанавливаем необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import lightgbm\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем датасет в переменной data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv(r'C:\\Users\\Хозяйка\\Проекты Практикума\\datasets\\toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдения:**\n",
    "- Корпус текстов на английском языке;\n",
    "- В данных содержится колонка с дублированными индексами, которую нужно будет удалить;\n",
    "- Пропущенных значений нет;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу **удаляем колонку 'Unnamed: 0'**. Скорее всего она добавилась в датасет из-за неправильной выгрузки информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'toxic'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь **создадим фукцию**, которая будет:  \n",
    "- чистить текст от ненужных символов;\n",
    "- удалит стоп-слова из текста;\n",
    "- лемматизирует текст;\n",
    "- приведет слова к нижнему регистру;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем модель, которая будет разбивать текст на токены\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clearing_n_lemmatizing(text):\n",
    "    \n",
    "    clear_text = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # перебираем токены\n",
    "    for token in doc:\n",
    "        \n",
    "        # оставляем только слова, исключая прочие символы\n",
    "        if token.is_alpha == True:\n",
    "            \n",
    "            # исключаем стоп-слова\n",
    "            if token.is_stop == False:\n",
    "                \n",
    "                # сохраняем в список лемматизированные слова и приводим их к нижнему регистру\n",
    "                clear_text.append(token.lemma_.lower())\n",
    "    \n",
    "    # объединяем список из очищенных слов в единую строку и выводим ее\n",
    "    return ' '.join(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation edit username hardcore metallica fan revert vandalism closure gas vote new york dolls fac remove template talk page retire'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем\n",
    "\n",
    "clearing_n_lemmatizing(data.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь **создадим новую колонку** в датафрейме с очищенными строками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12min 56s\n",
      "Wall time: 13min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['lemma'] = data.text.apply(lambda x: clearing_n_lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation edit username hardcore metallica f...\n",
       "1    match background colour seemingly stick thank ...\n",
       "2    hey man try edit war guy constantly remove rel...\n",
       "3    real suggestion improvement wonder section sta...\n",
       "4                        sir hero chance remember page\n",
       "Name: lemma, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем\n",
    "\n",
    "data.lemma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данные на признаки и таргет, а затем поделим их на обучающую, валидационную и тестовую выборки в соотношении 3:1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.lemma\n",
    "y = data.toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95574, 31859, 31859)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0], x_valid.shape[0], x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итоги:**\n",
    "\n",
    "- Мы очистили комментарии от лишних символов и стоп-слов;\n",
    "- Лемматизировали каждое слово;\n",
    "- Привели слова к нижнему регистру, чтобы слова написанные с заглавной буквы и нет обрабатывались машиной одинаково;\n",
    "- Сохранили в отдельные переменные признаки и таргет;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать две технологии приведения данных в векторный вид: TF-IDF векторизация и с помощью гриппировки. Используем трансформеры **TfidfVectorizer** и **CountVectorizer**\n",
    "\n",
    "Транформируем отдельно с помощью этих трансформеров тренировочную и тестовую выборку и сохраним в новые переменные. Добавим атрибут *stop_words*, чтобы трансформер дополнительно проверил наличие стоп-слов в текстах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', dtype=np.float32)\n",
    "counter = CountVectorizer(stop_words='english', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.31 s\n",
      "Wall time: 3.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# трансформируем данные\n",
    "\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_valid_tfidf = tfidf.transform(x_valid)\n",
    "x_test_tfidf = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<95574x109085 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 2080267 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.12 s\n",
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train_counted = counter.fit_transform(x_train)\n",
    "x_valid_counted = counter.transform(x_valid)\n",
    "x_test_counted = counter.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<95574x109085 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 2080267 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_counted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь подготовим модели к обучению на наших векторизованных данных. Будем использовать:  \n",
    "- **LogisticRegression**\n",
    "- **LGBMClassifier**\n",
    "- **RidgeClassifier**\n",
    "\n",
    "У всех моделей привоим атрибуту 'class_weight' значение 'balanced', потому что кол-во положительных и отрицательных объектов у нас различается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем первую модель: **LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.98 s\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_log1 = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "model_log1.fit(x_train_tfidf, y_train)\n",
    "pred = model_log1.predict(x_valid_tfidf)\n",
    "f1_tfidf = f1_score(pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.02 s\n",
      "Wall time: 2.87 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_log2 = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "model_log2.fit(x_train_counted, y_train)\n",
    "pred = model_log2.predict(x_valid_counted)\n",
    "f1_counted = f1_score(pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты модели LogisticRegression:\n",
      "\n",
      "С векторизацией TF-IDF: 0.7552278820375337\n",
      "C группирующей векторизацией: 0.7612939208031233\n"
     ]
    }
   ],
   "source": [
    "print(f'Результаты модели LogisticRegression:')\n",
    "print()\n",
    "print(f'С векторизацией TF-IDF: {f1_tfidf}')\n",
    "print(f'C группирующей векторизацией: {f1_counted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGBMClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9644, number of negative: 85930\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.534371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 396494\n",
      "[LightGBM] [Info] Number of data points in the train set: 95574, number of used features: 7927\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "CPU times: total: 34 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_lgbm1 = lightgbm.LGBMClassifier(class_weight='balanced')\n",
    "model_lgbm1.fit(x_train_tfidf, y_train)\n",
    "pred = model_lgbm1.predict(x_valid_tfidf)\n",
    "f1_tfidf = f1_score(pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9644, number of negative: 85930\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.459660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34764\n",
      "[LightGBM] [Info] Number of data points in the train set: 95574, number of used features: 7927\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "CPU times: total: 15.1 s\n",
      "Wall time: 6.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_lgbm2 = lightgbm.LGBMClassifier(class_weight='balanced')\n",
    "model_lgbm2.fit(x_train_counted, y_train)\n",
    "pred = model_lgbm2.predict(x_valid_counted)\n",
    "f1_counted = f1_score(pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты модели LGBMClassifier:\n",
      "\n",
      "С векторизацией TF-IDF: 0.7430229345122962\n",
      "C группирующей векторизацией: 0.7394356503785272\n"
     ]
    }
   ],
   "source": [
    "print(f'Результаты модели LGBMClassifier:')\n",
    "print()\n",
    "print(f'С векторизацией TF-IDF: {f1_tfidf}')\n",
    "print(f'C группирующей векторизацией: {f1_counted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3h 43min 57s\n",
      "Wall time: 39min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_cat1 = CatBoostClassifier(loss_function='Logloss', \n",
    "                           max_depth=8,\n",
    "                           auto_class_weights='Balanced', \n",
    "                           early_stopping_rounds=50,\n",
    "                           verbose=False)\n",
    "model_cat1.fit(x_train_tfidf, y_train)\n",
    "pred = model_cat1.predict(x_valid_tfidf)\n",
    "f1_tfidf = f1_score(pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 40min 22s\n",
      "Wall time: 12min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_cat2 = CatBoostClassifier(loss_function='Logloss', \n",
    "                           max_depth=8,\n",
    "                           auto_class_weights='Balanced', \n",
    "                           early_stopping_rounds=50,\n",
    "                           verbose=False)\n",
    "model_cat2.fit(x_train_counted, y_train)\n",
    "pred = model_cat2.predict(x_valid_counted)\n",
    "f1_counted = f1_score(pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты модели CatBoostRegressor:\n",
      "\n",
      "С векторизацией TF-IDF: 0.7636363636363637\n",
      "C группирующей векторизацией: 0.7600389050993469\n"
     ]
    }
   ],
   "source": [
    "print(f'Результаты модели CatBoostRegressor:')\n",
    "print()\n",
    "print(f'С векторизацией TF-IDF: {f1_tfidf}')\n",
    "print(f'C группирующей векторизацией: {f1_counted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь приступим к **тестированию лучших моделей**. У нас это:\n",
    "- CatBoostClassifier, обученная на данных с TF-IDF векторизацией;\n",
    "- CatBoostClassifier, обученная на данных с CountVectorizer;\n",
    "- LogisticRegression, обученная на данных с CountVectorizer;\n",
    "\n",
    "Все они подходят по параметру f1 > 0.75. Выберем ту, у которой будет результат лучше на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка предсказаний CatBoostClassifier (TfidfVectorizer): 0.7264021887824899\n",
      "Оценка предсказаний CatBoostClassifier (CountVectorizer): 0.7457286432160805\n",
      "Оценка предсказаний LogisticRegression (CountVectorizer): 0.7553114611938141\n"
     ]
    }
   ],
   "source": [
    "print(f'Оценка предсказаний CatBoostClassifier (TfidfVectorizer): {f1_score(model_cat1.predict(x_test_counted), y_test)}')\n",
    "print(f'Оценка предсказаний CatBoostClassifier (CountVectorizer): {f1_score(model_cat2.predict(x_test_counted), y_test)}')\n",
    "print(f'Оценка предсказаний LogisticRegression (CountVectorizer): {f1_score(model_log2.predict(x_test_counted), y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы проверили три модели для работы с категориальным целевым признаком. Итоговые наблюдения:  \n",
    "\n",
    "1) Модели обучаются быстрее на данных с CountVectorizer векторизацией. Однако практически во всех случаях качество предсказаний хуже, чем у TfIdfVectorizer.  \n",
    "\n",
    "2) Модель **LogisticRegression** - единственное исключение, потому что на ней векторизация CountVectorizer показала результат лучше, чем вторая. Также у нее лучший результат метрики F1 и самое быстрое время обучения.  \n",
    "\n",
    "3) Модель **LightGBM Classifier** имеет самый низкий результат по необходимой нам метрике, который не подходит нам по условию задачи (f1 > 0.75). Время обучения - маленькое.  \n",
    "\n",
    "4) **CatBoostClassifier**. Обе ее модели показали удовлетворительный результат на тренировочной выборке, но они обе имеют очень большое время обучения. Особенно та, которая училась на данных с TfIdfVectorizer векторизацией (почти 40 минут).  \n",
    "\n",
    "Мы проверили все три модели, которые показали удовлетворительный результат на валидационной выборке, были проверены на тестовой выборке. Единственная, у которой практически не упал показатель f1-метрики - это Логистическая регрессия. У остальных сильно упал результат.\n",
    " \n",
    "*Исходя из этих результатов, рекомендую к использованию модель LogisticRegression со сбалансированными классами с векторизацией через CountVectorizer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 4187,
    "start_time": "2023-09-16T11:55:11.119Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-16T11:56:11.339Z"
   },
   {
    "duration": 3807,
    "start_time": "2023-09-16T11:56:11.351Z"
   },
   {
    "duration": 2652,
    "start_time": "2023-09-16T11:56:15.160Z"
   },
   {
    "duration": 15,
    "start_time": "2023-09-16T11:56:17.813Z"
   },
   {
    "duration": 71,
    "start_time": "2023-09-16T11:56:17.830Z"
   },
   {
    "duration": 34,
    "start_time": "2023-09-16T11:56:17.903Z"
   },
   {
    "duration": 34,
    "start_time": "2023-09-16T11:56:17.939Z"
   },
   {
    "duration": 689,
    "start_time": "2023-09-16T11:56:17.974Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-16T11:56:18.665Z"
   },
   {
    "duration": 81,
    "start_time": "2023-09-16T11:56:18.670Z"
   },
   {
    "duration": 1197273,
    "start_time": "2023-09-16T11:56:18.753Z"
   },
   {
    "duration": 7,
    "start_time": "2023-09-16T12:16:16.029Z"
   },
   {
    "duration": 60,
    "start_time": "2023-09-16T12:16:16.043Z"
   },
   {
    "duration": 5,
    "start_time": "2023-09-16T12:16:16.105Z"
   },
   {
    "duration": 42,
    "start_time": "2023-09-16T12:16:16.111Z"
   },
   {
    "duration": 4866,
    "start_time": "2023-09-16T12:16:16.155Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-16T12:16:21.023Z"
   },
   {
    "duration": 4778,
    "start_time": "2023-09-16T12:16:21.029Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-16T12:16:25.808Z"
   },
   {
    "duration": 23,
    "start_time": "2023-09-16T12:16:42.997Z"
   },
   {
    "duration": 5131,
    "start_time": "2023-09-16T12:16:43.837Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-16T12:16:53.297Z"
   },
   {
    "duration": 4918,
    "start_time": "2023-09-16T12:16:55.679Z"
   },
   {
    "duration": 565,
    "start_time": "2023-09-16T12:17:22.984Z"
   },
   {
    "duration": 196,
    "start_time": "2023-09-16T12:17:40.530Z"
   },
   {
    "duration": 223249,
    "start_time": "2023-09-16T12:18:37.299Z"
   },
   {
    "duration": 86532,
    "start_time": "2023-09-16T12:22:36.925Z"
   },
   {
    "duration": 126016,
    "start_time": "2023-09-16T12:24:35.656Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-16T12:26:47.556Z"
   },
   {
    "duration": 86978,
    "start_time": "2023-09-16T12:28:10.168Z"
   },
   {
    "duration": 146410,
    "start_time": "2023-09-16T12:30:57.237Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-16T12:33:50.756Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-16T12:34:11.570Z"
   },
   {
    "duration": 252979,
    "start_time": "2023-09-16T12:34:29.209Z"
   },
   {
    "duration": 115580,
    "start_time": "2023-09-16T12:38:53.818Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-16T12:41:02.956Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
